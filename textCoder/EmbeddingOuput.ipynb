{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9332157b-2015-4ed9-8aad-9e706229bb16",
   "metadata": {},
   "source": [
    "# 根据bci2a数据集中的元信息，拿到能用于第一次clip实验的句向量\n",
    "## 代码来自metaData_Qw3_0.6B.ipynb\n",
    "## 11.2 第一次实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e421a2-6086-49f5-9c60-3b8b05cd7f42",
   "metadata": {},
   "source": [
    "## 1.加载模型与基础配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45232e77-6c99-42da-a456-2aee176cccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu128 | CUDA 可用: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "显存: 可用 78.84 GB / 总计 79.25 GB\n"
     ]
    }
   ],
   "source": [
    "#基本 GPU 信息\n",
    "import torch, os, subprocess, sys\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA 可用:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(f\"显存: 可用 {free/1024**3:.2f} GB / 总计 {total/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5c6295-954f-4d7f-b256-c5de9adbbc37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入常用库\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import random\n",
    "\n",
    "# 固定随机种子，保证 t-SNE 可视化尽量可复现\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# 选择设备（有 GPU 就用 GPU）\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70508d1c-2195-4eed-aed8-a9f1a7597af6",
   "metadata": {},
   "source": [
    "## 2.准备 BCI IV-2a 的四类英文描述（带指令）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b8c1c5-a2d5-4aba-a7e8-d05f982a7b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实验设置 ---\n",
      "受试者数量: 9\n",
      "动作类别数量: 4\n",
      "模板数量: 2\n",
      "--- 总计 ---\n",
      "总计生成的Prompt数量: 72 (预期: 72)\n",
      "对应的标签数量: 72\n",
      "对应的受试者ID数量: 72\n",
      "\n",
      "--- 示例: 受试者1 (A01) 的前4条Prompts ---\n",
      "  [受试者 1, 类别: left_hand]\n",
      "  Prompt: The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, imagined movement of the left hand\n",
      "\n",
      "  [受试者 1, 类别: left_hand]\n",
      "  Prompt: The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, motor imagery of the left hand\n",
      "\n",
      "  [受试者 1, 类别: right_hand]\n",
      "  Prompt: The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, imagined movement of the right hand\n",
      "\n",
      "  [受试者 1, 类别: right_hand]\n",
      "  Prompt: The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, motor imagery of the right hand\n",
      "\n",
      "--- 示例: 受试者3 (A03) 的第一条Prompt ---\n",
      "  [受试者 3, 类别: left_hand]\n",
      "  Prompt: The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 26-year-old male subject, imagined movement of the left hand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ================= 1. 实验设置 =================\n",
    "\n",
    "# 统一的英文指令（保持不变）\n",
    "INSTRUCT = (\n",
    "    \"The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. \"\n",
    "    \"Produce an embedding for retrieval with a strong emphasis on LATERALITY: \"\n",
    "    \"if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; \"\n",
    "    \"if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. \"\n",
    "    \"Do not include any movement for the non-mentioned hand.\"\n",
    ")\n",
    "\n",
    "# --- 受试者特定信息 (9个受试者) ---\n",
    "# A01T 到 A09T 的年龄\n",
    "ages_subject = [22, 24, 26, 24, 24, 23, 25, 23, 27] # 注意：最后一个受试者A09的年龄是27岁\n",
    "# A01T 到 A09T 的性别 (0=male, 1=female)\n",
    "gender_subject = [1, 1, 0, 1, 0, 1, 0, 0, 0] \n",
    "# 性别数字到文本的映射\n",
    "GENDERS_MAP = [\"male \", \"female \"] \n",
    "\n",
    "# --- 动作模板 ---\n",
    "TEMPLATES = [\n",
    "    \"imagined movement of the {label}\",   # 动作句式1\n",
    "    \"motor imagery of the {label}\",       # 动作句式2\n",
    "]\n",
    "\n",
    "# --- 标签映射 ---\n",
    "label_text = {\n",
    "    \"left_hand\":  \"left hand\",\n",
    "    \"right_hand\": \"right hand\",\n",
    "    \"feet\":       \"both feet\",\n",
    "    \"tongue\":     \"tongue\",\n",
    "}\n",
    "\n",
    "# ================== 2. 生成 Prompt ==================\n",
    "\n",
    "# 预备四个并行列表\n",
    "all_texts, all_labels, all_subjects = [], [], []\n",
    "\n",
    "# 遍历9个受试者\n",
    "# (使用 zip 将年龄和性别列表打包在一起)\n",
    "for i, (age, gender_idx) in enumerate(zip(ages_subject, gender_subject)):\n",
    "    subject_id = i + 1  # 受试者编号 (1 到 9)\n",
    "    gender_str = GENDERS_MAP[gender_idx] # 将 0/1 转换成 \"male \"/\"female \"\n",
    "    \n",
    "    # 遍历4个动作类别\n",
    "    for cls, phrase in label_text.items():\n",
    "        \n",
    "        # 遍历2个动作模板\n",
    "        for tpl in TEMPLATES:\n",
    "            # 构造一条完整句子: A {age}-year-old {gender}subject, + {动作模块}\n",
    "            sent = f\"A {age}-year-old {gender_str}subject, \" + tpl.format(label=phrase)\n",
    "            \n",
    "            # 加上统一指令\n",
    "            final_prompt = INSTRUCT + \" \" + sent\n",
    "            \n",
    "            # --- 收集结果 ---\n",
    "            all_texts.append(final_prompt)   # 存储最终的Prompt\n",
    "            all_labels.append(cls)           # 存储对应的类别标签\n",
    "            all_subjects.append(subject_id)  # 存储对应的受试者ID\n",
    "\n",
    "# ================== 3. 结果检查 ==================\n",
    "\n",
    "# 总Prompt数 = 9个受试者 × 4个类别 × 2个模板 = 72\n",
    "print(f\"--- 实验设置 ---\")\n",
    "print(f\"受试者数量: {len(ages_subject)}\")\n",
    "print(f\"动作类别数量: {len(label_text)}\")\n",
    "print(f\"模板数量: {len(TEMPLATES)}\")\n",
    "print(f\"--- 总计 ---\")\n",
    "print(f\"总计生成的Prompt数量: {len(all_texts)} (预期: {len(ages_subject) * len(label_text) * len(TEMPLATES)})\")\n",
    "print(f\"对应的标签数量: {len(all_labels)}\")\n",
    "print(f\"对应的受试者ID数量: {len(all_subjects)}\\n\")\n",
    "\n",
    "\n",
    "print(\"--- 示例: 受试者1 (A01) 的前4条Prompts ---\")\n",
    "# 受试者1有 4(类别) * 2(模板) = 8 条prompt，我们只看前4条\n",
    "for i in range(4):\n",
    "    print(f\"  [受试者 {all_subjects[i]}, 类别: {all_labels[i]}]\")\n",
    "    print(f\"  Prompt: {all_texts[i]}\\n\")\n",
    "\n",
    "print(\"--- 示例: 受试者3 (A03) 的第一条Prompt ---\")\n",
    "# 受试者1有8条, 受试者2有8条, 所以受试者3从索引16开始\n",
    "idx_subject_3 = (3 - 1) * len(label_text) * len(TEMPLATES)\n",
    "print(f\"  [受试者 {all_subjects[idx_subject_3]}, 类别: {all_labels[idx_subject_3]}]\")\n",
    "print(f\"  Prompt: {all_texts[idx_subject_3]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccabf47-28db-4ca6-b893-44693cc80f2f",
   "metadata": {},
   "source": [
    "## 3.加载 Qwen3-Embedding-0.6B 并批量编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2116c7-1942-4d02-a3cf-fce9015fbbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 维度: 1024\n",
      "总文本: 72 | 示例前3条：\n",
      "- The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, imagined movement of the left hand\n",
      "- The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, motor imagery of the left hand\n",
      "- The subject is seated comfortably in an armchair facing a computer screen, performing motor imagery tasks. Produce an embedding for retrieval with a strong emphasis on LATERALITY: if the text mentions the LEFT hand, assume exclusively left-hand imagery with NO right-hand involvement; if the text mentions the RIGHT hand, assume exclusively right-hand imagery with NO left-hand involvement. Do not include any movement for the non-mentioned hand. A 22-year-old female subject, imagined movement of the right hand\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a88586810a5449888f79cc46408ca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句向量形状: (72, 1024)\n"
     ]
    }
   ],
   "source": [
    "# ================== 加载模型（保持不变） ==================\n",
    "# 加载 Qwen3-Embedding-0.6B\n",
    "model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "# 说明：\n",
    "# - SentenceTransformer 会自动处理 tokenizer + pooling，并支持 normalize_embeddings=True\n",
    "# - 需较新版本的 transformers>=4.51.0、sentence-transformers>=2.7.0\n",
    "model = SentenceTransformer(model_name, device=DEVICE)\n",
    "\n",
    "# （可选）查看向量维度，便于确认模型输出维数\n",
    "print(\"Embedding 维度:\", model.get_sentence_embedding_dimension())\n",
    "\n",
    "# ================== 直接使用你已生成的 all_texts / all_labels / all_ages / all_genders ==================\n",
    "# 此处不再从 prompts 重建；all_texts 等已在你上一个代码块中构造完毕\n",
    "print(f\"总文本: {len(all_texts)} | 示例前3条：\")\n",
    "for i in range(min(3, len(all_texts))):\n",
    "    print(\"-\", all_texts[i])\n",
    "\n",
    "# ================== 批量编码（保持参数风格一致） ==================\n",
    "embeddings = model.encode(\n",
    "    all_texts,\n",
    "    batch_size=32,                 # 可按显存调大/调小\n",
    "    normalize_embeddings=True,     # 输出向量做 L2 归一化（与余弦相似度匹配）\n",
    "    convert_to_numpy=True,         # 直接拿到 numpy 数组\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"句向量形状:\", embeddings.shape)  # 形如 (N, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b615ba4-6d04-438c-8817-693e266cda1d",
   "metadata": {},
   "source": [
    "## 4.合并T1，T2, 输出余弦相似距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93a2388-e4b5-4cf6-8f07-6387343e6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "     计算 36 个受试者特定的“平均原型向量”     \n",
      "==================================================\n",
      "原型向量将保存在: ./subject_prototypes/\n",
      "\n",
      "成功计算并保存了 36 个原型向量。\n",
      "\n",
      "==================================================\n",
      "     示例：受试者1 (S01) 的原型间余弦距离     \n",
      "==================================================\n",
      "            left_hand  right_hand    feet  tongue\n",
      "left_hand     -0.0000      0.0367  0.0503  0.0908\n",
      "right_hand     0.0367     -0.0000  0.0662  0.1022\n",
      "feet           0.0503      0.0662 -0.0000  0.1091\n",
      "tongue         0.0908      0.1022  0.1091  0.0000\n",
      "\n",
      "(提示：对角线为0.0，值越大表示区分度越好)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1. 定义工具函数 (l2norm) ---\n",
    "def l2norm(x, axis=-1, eps=1e-12):\n",
    "    \"\"\"对Numpy数组进行L2归一化\"\"\"\n",
    "    return x / (np.linalg.norm(x, axis=axis, keepdims=True) + eps)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"     计算 36 个受试者特定的“平均原型向量”     \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 2. 创建文件夹用于保存 ---\n",
    "output_dir = \"subject_prototypes\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"原型向量将保存在: ./{output_dir}/\\n\")\n",
    "\n",
    "# --- 3. 准备工作 ---\n",
    "# (确保 all_labels 和 all_subjects 是Numpy数组，以便于索引)\n",
    "labels_np = np.array(all_labels)\n",
    "subjects_np = np.array(all_subjects)\n",
    "\n",
    "# 预备一个字典来存储36个原型\n",
    "prototypes = {}\n",
    "prototype_labels = [] # 用于打印余弦矩阵\n",
    "\n",
    "# --- 4. 循环计算 9x4=36 个原型 ---\n",
    "for subject_id in range(1, 10): # 遍历受试者 1 到 9\n",
    "    for cls in label_text.keys(): # 遍历 4 个类别\n",
    "        \n",
    "        # 找到所有符合条件的向量的索引\n",
    "        # (不区分T1或T2模板)\n",
    "        indices = np.where(\n",
    "            (subjects_np == subject_id) & (labels_np == cls)\n",
    "        )[0]\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            print(f\"警告：未找到 受试者{subject_id} - 类别{cls} 的向量，跳过...\")\n",
    "            continue\n",
    "            \n",
    "        # --- 核心步骤: 计算平均原型 ---\n",
    "        # 1. 从 'embeddings' 中提取所有相关向量\n",
    "        # 2. .mean(axis=0) 在向量空间中计算均值\n",
    "        # 3. l2norm(...) 重新归一化，确保它是一个单位向量\n",
    "        proto_vector = l2norm(embeddings[indices].mean(axis=0, keepdims=True))[0]\n",
    "        \n",
    "        # 将Numpy数组转换为PyTorch张量\n",
    "        proto_tensor = torch.tensor(proto_vector)\n",
    "        \n",
    "        # --- 5. 存储原型 ---\n",
    "        key = f\"S{subject_id:02d}_{cls}\" # 例如: \"S01_left_hand\"\n",
    "        prototypes[key] = proto_tensor\n",
    "        prototype_labels.append(key)\n",
    "        \n",
    "        # 保存为单独的 .pt 文件\n",
    "        save_path = os.path.join(output_dir, f\"{key}.pt\")\n",
    "        torch.save(proto_tensor, save_path)\n",
    "\n",
    "print(f\"成功计算并保存了 {len(prototypes)} 个原型向量。\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 6. 简单输出余弦距离 (以受试者1为例) ---\n",
    "print(\"=\"*50)\n",
    "print(\"     示例：受试者1 (S01) 的原型间余弦距离     \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 提取受试者1的4个原型向量\n",
    "s1_keys = [f\"S01_{cls}\" for cls in label_text.keys()]\n",
    "s1_vectors = [prototypes[key].numpy() for key in s1_keys] # 转回Numpy以便于sklearn计算\n",
    "\n",
    "\n",
    "# 使用 cosine_similarity 计算 4x4 的相似度矩阵\n",
    "# (注意: 1.0 - 相似度 = 余弦距离)\n",
    "cos_sim_matrix = cosine_similarity(s1_vectors)\n",
    "cos_dist_matrix = 1.0 - cos_sim_matrix\n",
    "\n",
    "# 将结果用Pandas美化输出\n",
    "df_dist = pd.DataFrame(\n",
    "    cos_dist_matrix,\n",
    "    columns=label_text.keys(),\n",
    "    index=label_text.keys()\n",
    ")\n",
    "\n",
    "print(df_dist.round(4))\n",
    "\n",
    "print(\"\\n(提示：对角线为0.0，值越大表示区分度越好)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ce79cd-7a0a-4a6a-973d-5566ff1f620e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m维度\u001b[39m\u001b[33m\"\u001b[39m,(\u001b[43mprototypes\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m).shape)\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "print(\"维度\",(prototypes[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f487a38-520f-44b7-899e-c4527677c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch_env)",
   "language": "python",
   "name": "test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
